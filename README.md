<h1>Udacity - Data Engineering Nanodgree Program</h1>

#### Introduction
In this program, we'll learn to design data models, build data warehouses and data lakes, automate data pipelines, and work with massive datasets. At the end of the program, we’ll combine all skills by completing a capstone project.
More informations about Udacity's Data Engineering Nanodegree Program [here](https://www.udacity.com/course/data-engineer-nanodegree--nd027). 



<h3>Projects</h3>
<h4>Project 1 Data modeling with Postgres</h5>
In this project, we’ll model user activity data for a music streaming app called Sparkify. We’ll create a relational database and ETL pipeline designed to optimize queries for understanding what songs users are listening to. In PostgreSQL we will also define Fact and Dimension tables and insert data into your new tables.

<h4>Project 2 Data Modeling with Apache Cassandra</h5>
In this project, we’ll model user activity data for a music streaming app called Sparkify. We’ll create a noSQL database and ETL pipeline designed to optimize queries for understanding what songs users are listening to. We’ll model your data in Apache Cassandra to allow for specific queries provided by the analytics team at Sparkify.

<h4>Project 3 Cloud Data Warehouse</h5>
In this project, we are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.


<h4>Project 4 Spark and Data Lakes</h5>
In this project, we'll build an ETL pipeline for a data lake. The data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in the app. We will load data from S3, process the data into analytics tables using Spark, and load them back into S3. We'll deploy this Spark process on a cluster using AWS.

<h4>Project 5 Data Pipelines with Airflow</h5>
In this project, we’ll continue the work on the music streaming company’s data infrastructure by creating and automating a set of data pipelines. We’ll configure and schedule data pipelines with Airflow and monitor and debug production pipelines.

<h4>Project 6 Capstone Project</h5>
The purpose of the data engineering capstone project is to combine what we've learned throughout the program. We'll define the scope of the project and the data we'll be working with. We'll gather data from several different data sources; transform, combine, and summarize it; and create a clean database for others to analyze.

<h3>Educational Objectives</h3>
<ul>
  <li> Create user-friendly relational and NoSQL data models</li>
  <li> Create scalable and efficient data warehouses</li>
  <li>Work efficiently with massive datasets</li>
  <li>Build and interact with a cloud-based data lake</li>
  <li>Automate and monitor data pipelines</li>
  <li>Develop proficiency in Spark, Airflow, and AWS tools</li>
</ul> 



